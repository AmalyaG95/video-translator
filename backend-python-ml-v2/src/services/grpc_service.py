"""
gRPC Service Implementation

Follows best-practices/00-ARCHITECTURE.md communication patterns.
Maintains compatibility with existing NestJS API.
"""

import asyncio
import grpc
import json
import logging
import re
import subprocess
import tempfile
from typing import Iterator, Dict, Any, Optional, List
from datetime import datetime
from collections import deque
from pathlib import Path

# Proto imports will be generated at runtime
# For now, import from proto directory
try:
    from ..proto import translation_pb2, translation_pb2_grpc
except ImportError:
    # Fallback: proto files will be generated by entrypoint script
    import sys
    from pathlib import Path
    proto_path = Path(__file__).parent.parent / "proto"
    sys.path.insert(0, str(proto_path))
    import translation_pb2
    import translation_pb2_grpc
from ..pipeline import get_pipeline_orchestrator
from .session_manager import get_session_manager, SessionStatus
from ..app_logging import get_logger
from ..utils import get_path_resolver
from ..core import get_resource_manager, get_model_manager
from ..core.language_detector import detect_language_from_text

logger = get_logger("grpc_service")

# Global log buffers per session (last 50 logs per session)
_log_buffers: Dict[str, deque] = {}
_MAX_LOG_BUFFER_SIZE = 50
# Track which logs have been sent to avoid duplicates
_sent_log_ids: Dict[str, set] = {}


class SessionLogHandler(logging.Handler):
    """Log handler that captures logs for a specific session."""
    
    def __init__(self, session_id: str):
        super().__init__()
        self.session_id = session_id
    
    def emit(self, record: logging.LogRecord):
        """Capture log record and add to session buffer."""
        try:
            # Only capture logs for this session
            record_session_id = getattr(record, "session_id", None)
            if record_session_id and record_session_id != self.session_id:
                return  # Skip logs from other sessions
            
            # If no session_id in record, assume it's for this session (common case)
            # This allows us to capture logs that don't explicitly set session_id
            
            # Get log level name
            level_name = record.levelname.lower()
            if level_name == "critical":
                level_name = "error"
            
            # Extract stage from logger name or record
            stage = getattr(record, "stage", None) or ""
            if not stage and record.name:
                # Try to extract stage from logger name (e.g., "stage.transcription")
                parts = record.name.split(".")
                if len(parts) > 1 and parts[0] == "stage":
                    stage = parts[1]
            
            # Extract chunk_id
            chunk_id = getattr(record, "chunk_id", None) or ""
            
            # Extract extra_data
            extra_data = {}
            if hasattr(record, "extra_data"):
                extra_data = record.extra_data
            elif hasattr(record, "__dict__"):
                # Get all non-standard attributes
                standard_attrs = {
                    "name", "msg", "args", "created", "filename", "funcName",
                    "levelname", "levelno", "lineno", "module", "msecs",
                    "message", "pathname", "process", "processName", "relativeCreated",
                    "thread", "threadName", "exc_info", "exc_text", "stack_info",
                    "session_id", "stage", "chunk_id", "extra_data"
                }
                extra_data = {
                    k: v for k, v in record.__dict__.items()
                    if k not in standard_attrs and not k.startswith("_")
                }
            
            # Filter out technical/debug messages that users don't need to see
            message = record.getMessage()
            
            # Only filter messages that EXACTLY start with technical prefixes
            # This ensures user-facing messages like "Stage started: audio_extraction" pass through
            message_lower = message.lower()
            
            # List of technical prefixes that should be filtered (must match start of message exactly)
            technical_prefixes_to_filter = [
                # "[grpc stream]",  # Removed - user wants to see these messages
                # "yielding progress to nestjs",  # Removed - user wants to see these messages
                "sending to nestjs",
                # "â†’ yielding progress to nestjs",  # Removed - user wants to see these messages
                "[transcription progress] calling progress_callback",
                "calling whisper model.transcribe()",
                "checkpoint saved:",
                "detecting language for file:",
                "using standard transcription settings",
                "using fast transcription settings",
                "whisper transcription initialized",
                "model initialization stage complete",
                "executing stage:",  # Note: "Stage started:" is different and should pass
                "grpc server started",
                "checkpoint manager initialized",
                "cleanup manager initialized",
                "model manager initialized",
                "path resolver initialized",
                "pipeline orchestrator initialized",
                "quality validator initialized",
                "resource manager initialized",
                "periodic cleanup started",
                "opentelemetry tracing configured",
                "translation servicer initialized",
                "[log handler]",
                "[log entries]",
            ]
            
            # Only filter if message starts with one of these exact prefixes
            should_filter = False
            for prefix in technical_prefixes_to_filter:
                if message_lower.startswith(prefix):
                    should_filter = True
                    break
            
            # Don't filter "[GRPC STREAM]" or "yielding progress to nestjs" messages - user wants to see them
            
            if should_filter:
                return  # Don't add this log to the buffer
            
            # Filter out technical/internal fields that users don't need
            technical_fields = {
                "update_dict_keys",
                "log_entries_count",
                "extra_data",
                "preserved_fields",
                "updated_fields",
            }
            filtered_extra_data = {
                k: v for k, v in extra_data.items()
                if k not in technical_fields and not k.startswith("_")
            }
            
            # Clean up message - remove file paths for user-friendly display
            # Replace file paths with generic descriptions
            cleaned_message = message
            if "/app/uploads/" in cleaned_message or "/app/temp_work/" in cleaned_message:
                # Remove file paths from messages
                cleaned_message = re.sub(r'/app/uploads/[^\s]+', 'video file', cleaned_message)
                cleaned_message = re.sub(r'/app/temp_work/[^\s]+', 'working directory', cleaned_message)
                cleaned_message = re.sub(r'-> working directory', '', cleaned_message).strip()
                # Clean up "Audio extracted successfully: working directory" -> "Audio extracted successfully"
                cleaned_message = re.sub(r':\s*working directory\s*$', '', cleaned_message).strip()
            
            # Create log entry (only include extra_data if there are user-relevant fields)
            log_entry = {
                "timestamp": datetime.utcnow().isoformat() + "Z",
                "level": level_name,
                "stage": stage,
                "message": cleaned_message,
                "chunk_id": str(chunk_id) if chunk_id else "",
                "session_id": self.session_id,
                "extra_data": json.dumps(filtered_extra_data, default=str) if filtered_extra_data else "",
            }
            
            # Add to buffer
            if self.session_id not in _log_buffers:
                _log_buffers[self.session_id] = deque(maxlen=_MAX_LOG_BUFFER_SIZE)
            
            _log_buffers[self.session_id].append(log_entry)
            
            
        except Exception as e:
            # Don't let log handler errors break the application
            pass


class TranslationServicer(translation_pb2_grpc.TranslationServiceServicer):
    """
    gRPC servicer for video translation.
    
    Maintains compatibility with existing NestJS API.
    """
    
    def __init__(self):
        """Initialize translation servicer."""
        self.pipeline = get_pipeline_orchestrator()
        self.session_manager = get_session_manager()
        self.path_resolver = get_path_resolver()
        self.resource_manager = get_resource_manager()
        
        logger.info("Translation servicer initialized")
    
    def _get_log_entries(self, session_id: str, only_new: bool = True) -> List[translation_pb2.LogEntry]:
        """
        Get log entries for a session.
        
        Args:
            session_id: Session identifier
            only_new: If True, only return logs that haven't been sent yet (deduplication)
        
        Returns:
            List of log entries
        """
        if session_id not in _log_buffers:
            return []
        
        log_buffer = _log_buffers[session_id]
        log_entries = []
        
        # Initialize sent log IDs set for this session if not exists
        if session_id not in _sent_log_ids:
            _sent_log_ids[session_id] = set()
        
        sent_ids = _sent_log_ids[session_id]
        
        for log_data in log_buffer:
            # Create unique ID for this log entry (timestamp + message)
            log_id = f"{log_data['timestamp']}:{log_data['message']}"
            
            # Skip if we only want new logs and this one was already sent
            if only_new and log_id in sent_ids:
                continue
            
            log_entry = translation_pb2.LogEntry(
                timestamp=log_data["timestamp"],
                level=log_data["level"],
                stage=log_data["stage"],
                message=log_data["message"],
                chunk_id=log_data["chunk_id"],
                session_id=log_data["session_id"],
                extra_data=log_data["extra_data"],
            )
            log_entries.append(log_entry)
            
            # Mark this log as sent
            sent_ids.add(log_id)
        
        return log_entries
    
    def _setup_log_handler(self, session_id: str):
        """Setup log handler for a session."""
        # Create session-specific log handler
        handler = SessionLogHandler(session_id)
        handler.setLevel(logging.INFO)
        
        # Add handler to root logger to capture all logs
        root_logger = logging.getLogger()
        root_logger.addHandler(handler)
        
        return handler
    
    def _remove_log_handler(self, handler: logging.Handler):
        """Remove log handler."""
        root_logger = logging.getLogger()
        root_logger.removeHandler(handler)
    
    async def TranslateVideo(
        self,
        request: translation_pb2.TranslationRequest,
        context: grpc.aio.ServicerContext,
    ) -> Iterator[translation_pb2.TranslationProgress]:
        """
        Stream translation progress.
        
        Maintains compatibility with existing NestJS API.
        """
        session_id = request.session_id
        
        logger.info(
            f"TranslateVideo called: source_lang={request.source_lang}, target_lang={request.target_lang}",
            session_id=session_id,
            extra_data={
                "file_path": request.file_path,
                "source_lang": request.source_lang,
                "target_lang": request.target_lang,
            },
        )
        
        # Validate and log language codes
        if request.target_lang != 'hy' and request.target_lang == 'zh':
            logger.warning(
                f"WARNING: Received target_lang='zh' (Chinese) but expected 'hy' (Armenian). "
                f"This may indicate a frontend/backend language mismatch.",
                session_id=session_id,
                extra_data={
                    "received_target_lang": request.target_lang,
                    "expected_target_lang": "hy",
                    "source_lang": request.source_lang,
                }
        )
        
        try:
            # Create session
            voice_gender = getattr(request, 'voice_gender', 'neutral') or 'neutral'
            session = self.session_manager.create_session(
                session_id,
                request.file_path,
                request.source_lang,
                request.target_lang,
                voice_gender=voice_gender,
            )
            
            # Setup log handler for this session
            log_handler = self._setup_log_handler(session_id)
            
            # Get cancellation event
            cancellation_event = self.session_manager.get_cancellation_event(session_id)
            
            # Progress queue for streaming
            progress_queue = asyncio.Queue()
            DONE = object()
            
            # State to preserve detailed fields across progress updates
            # This ensures fields like stage_number, total_stages persist even when stages don't pass them
            progress_state: Dict[str, Any] = {}
            
            async def run_pipeline():
                """Run pipeline in background task."""
                try:
                    # Get output path
                    artifacts = self.path_resolver.get_session_artifacts(session_id)
                    output_path = artifacts["translated_video"]
                    
                    # Progress callback with field preservation
                    async def progress_callback(
                        progress: float,
                        message: str,
                        stage: Optional[str] = None,
                        session_id: Optional[str] = None,
                        **kwargs: Any,
                    ):
                        """Put progress updates in queue, preserving detailed fields from previous updates."""
                        try:
                            # Merge new update with preserved state
                            # Only update fields that are provided (not None/empty), preserve others
                            new_update = {
                                "progress": progress,
                                "current_step": message,
                                "status": "processing",
                                "message": message,
                                "stage": stage or "",
                                **kwargs,
                            }
                            
                            # List of detailed fields that should be preserved if not provided
                            detailed_fields = [
                                "stage_number",
                                "total_stages",
                                "segments_processed",
                                "current_time",
                                "current_time_formatted",
                                "total_duration",
                                "total_duration_formatted",
                                "progress_percent",
                                "elapsed_time",
                                "current_chunk",
                                "total_chunks",
                                "early_preview_available",
                                "early_preview_path",
                            ]
                            
                            # Merge: use new values if provided, otherwise preserve from state
                            merged_update = progress_state.copy()
                            preserved_fields = []
                            updated_fields = []
                            
                            for key, value in new_update.items():
                                # For detailed fields: only update if explicitly provided (not None)
                                # For other fields: always update
                                if key in detailed_fields:
                                    # Only update detailed fields if value is explicitly provided (not None)
                                    if value is not None:
                                        merged_update[key] = value
                                        updated_fields.append(f"{key}={value}")
                                    else:
                                        # Preserving existing value
                                        if key in merged_update:
                                            preserved_fields.append(f"{key}={merged_update[key]}")
                                elif key == "progress":
                                    # Progress should only increase (never go backwards)
                                    current_progress = merged_update.get("progress", 0.0)
                                    if isinstance(value, (int, float)) and value > current_progress:
                                        merged_update[key] = value
                                    elif current_progress > 0:
                                        # Keep current progress if new value is lower
                                        preserved_fields.append(f"progress={current_progress}")
                                    else:
                                        # If no current progress, use new value
                                        merged_update[key] = value
                                else:
                                    # For other non-detailed fields (message, current_step, etc.), always update
                                    merged_update[key] = value
                            
                            # Update preserved state with merged values
                            progress_state.update(merged_update)
                            
                            # Log field preservation for debugging (debug level to avoid cluttering frontend)
                            # Only log when significant fields change to avoid spam
                            if (preserved_fields or updated_fields) and (
                                "stage_number" in str(updated_fields) or 
                                "segments_processed" in str(updated_fields) or
                                "current_time_formatted" in str(updated_fields)
                            ):
                                logger.debug(
                                    f"Progress update: {progress}% | stage={stage or 'N/A'} | "
                                    f"stage_number={merged_update.get('stage_number', 'N/A')}/{merged_update.get('total_stages', 'N/A')} | "
                                    f"segments={merged_update.get('segments_processed', 0)} | "
                                    f"time={merged_update.get('current_time_formatted', 'N/A')}/{merged_update.get('total_duration_formatted', 'N/A')}",
                                    session_id=session_id,
                                    stage=stage or "",
                                    extra_data={
                                        "progress": progress,
                                        "stage": stage or "",
                                        "updated_fields": updated_fields[:3] if updated_fields else [],  # Limit to first 3
                                        "preserved_fields": preserved_fields[:3] if preserved_fields else [],  # Limit to first 3
                                        "stage_number": merged_update.get("stage_number"),
                                        "total_stages": merged_update.get("total_stages"),
                                        "segments_processed": merged_update.get("segments_processed"),
                                        "current_time_formatted": merged_update.get("current_time_formatted"),
                                    }
                                )
                            
                            await progress_queue.put(merged_update)
                        except Exception as e:
                            logger.error(
                                "Error in progress callback",
                                exc_info=True,
                                extra_data={"error": str(e)},
                            )
                    
                    # Update session status
                    self.session_manager.update_session(
                        session_id,
                        status=SessionStatus.PROCESSING,
                        progress=0.0,
                        current_step="Starting pipeline",
                    )
                    
                    # Run pipeline
                    result = await self.pipeline.process_video(
                        video_path=Path(request.file_path),
                        output_path=output_path,
                        source_lang=request.source_lang,
                        target_lang=request.target_lang,
                        session_id=session_id,
                        voice_gender=voice_gender,
                        progress_callback=progress_callback,
                        cancellation_event=cancellation_event,
                    )
                    
                    # Final progress update (preserve detailed fields)
                    if result.get("success"):
                        final_update = progress_state.copy()
                        final_update.update({
                            "progress": 100.0,
                            "current_step": "Completed",
                            "status": "completed",
                            "message": "Translation completed successfully",
                        })
                        await progress_queue.put(final_update)
                        self.session_manager.update_session(
                            session_id,
                            status=SessionStatus.COMPLETED,
                            progress=100.0,
                            current_step="Completed",
                            result=result,
                        )
                    else:
                        final_update = progress_state.copy()
                        final_update.update({
                            "progress": 0.0,
                            "current_step": "Failed",
                            "status": "failed",
                            "message": result.get("error", "Unknown error"),
                        })
                        await progress_queue.put(final_update)
                        self.session_manager.update_session(
                            session_id,
                            status=SessionStatus.FAILED,
                            progress=0.0,
                            current_step="Failed",
                            error=result.get("error"),
                        )
                    
                except asyncio.CancelledError:
                    final_update = progress_state.copy()
                    final_update.update({
                        "progress": 0.0,
                        "current_step": "Cancelled",
                        "status": "cancelled",
                        "message": "Translation cancelled",
                    })
                    await progress_queue.put(final_update)
                    self.session_manager.update_session(
                        session_id,
                        status=SessionStatus.CANCELLED,
                        current_step="Cancelled",
                    )
                except Exception as e:
                    logger.error(
                        "Pipeline execution failed",
                        session_id=session_id,
                        exc_info=True,
                        extra_data={"error": str(e)},
                    )
                    final_update = progress_state.copy()
                    final_update.update({
                        "progress": 0.0,
                        "current_step": "Failed",
                        "status": "failed",
                        "message": str(e),
                    })
                    await progress_queue.put(final_update)
                    self.session_manager.update_session(
                        session_id,
                        status=SessionStatus.FAILED,
                        error=str(e),
                    )
                finally:
                    await progress_queue.put(DONE)
            
            try:
                # Start pipeline in background
                logger.info(
                    f"ðŸš€ [GRPC STREAM] Starting pipeline for session {session_id}",
                    session_id=session_id,
                )
                asyncio.create_task(run_pipeline())
                
                logger.info(
                    f"ðŸ“¡ [GRPC STREAM] Starting to stream progress updates for session {session_id}",
                    session_id=session_id,
                )
                
                # Stream progress updates
                while True:
                    try:
                        update = await asyncio.wait_for(progress_queue.get(), timeout=1.0)
                        
                        if update is DONE:
                            break
                        
                        # Get recent log entries
                        log_entries = self._get_log_entries(session_id)
                        
                        # Create progress response with all detailed fields
                        progress_response = translation_pb2.TranslationProgress(
                            session_id=session_id,
                            progress=update.get("progress", 0.0),
                            current_step=update.get("current_step", ""),
                            status=update.get("status", "processing"),
                            message=update.get("message", ""),
                            early_preview_available=update.get("early_preview_available", False),
                            early_preview_path=update.get("early_preview_path", ""),
                            current_chunk=update.get("current_chunk", 0),
                            total_chunks=update.get("total_chunks", 0),
                            log_entries=log_entries,
                            # Detailed progress fields
                            stage=update.get("stage", ""),
                            stage_number=update.get("stage_number", 0),
                            total_stages=update.get("total_stages", 0),
                            segments_processed=update.get("segments_processed", 0),
                            current_time=update.get("current_time", 0.0),
                            current_time_formatted=update.get("current_time_formatted", ""),
                            total_duration=update.get("total_duration", 0.0),
                            total_duration_formatted=update.get("total_duration_formatted", ""),
                            progress_percent=update.get("progress_percent", 0.0),
                            elapsed_time=update.get("elapsed_time", 0.0),
                        )
                        
                        # Log detailed fields being sent to NestJS (INFO level to see what's being sent)
                        # Only log when stage is transcription or when significant fields are present
                        should_log = (
                            update.get("stage") == "transcription" or
                            progress_response.stage_number > 0 or 
                            progress_response.segments_processed > 0 or
                            progress_response.current_time_formatted
                        )
                        if should_log:
                            logger.info(
                                f"ðŸ“¤ [GRPC STREAM] â†’ Yielding progress to NestJS: {progress_response.progress}% | "
                                f"stage={progress_response.stage} ({progress_response.stage_number}/{progress_response.total_stages}) | "
                                f"segments={progress_response.segments_processed} | "
                                f"time={progress_response.current_time_formatted}/{progress_response.total_duration_formatted} | "
                                f"log_entries={len(log_entries)}",
                                session_id=session_id,
                                stage=update.get("stage", ""),
                                extra_data={
                                    "progress": progress_response.progress,
                                    "stage": progress_response.stage,
                                    "stage_number": progress_response.stage_number,
                                    "total_stages": progress_response.total_stages,
                                    "segments_processed": progress_response.segments_processed,
                                    "current_time_formatted": progress_response.current_time_formatted,
                                    "total_duration_formatted": progress_response.total_duration_formatted,
                                    "progress_percent": progress_response.progress_percent,
                                    "log_entries_count": len(log_entries),
                                    "update_dict_keys": list(update.keys()),
                                }
                        )
                        
                        yield progress_response
                        
                    except asyncio.TimeoutError:
                        # Send heartbeat to keep connection alive (with recent logs and preserved fields)
                        log_entries = self._get_log_entries(session_id)
                        # Use preserved state for heartbeat to maintain all detailed fields
                        heartbeat_update = progress_state.copy()
                        heartbeat_update.update({
                            "current_step": "Processing...",
                            "status": "processing",
                        })
                        yield translation_pb2.TranslationProgress(
                            session_id=session_id,
                            progress=heartbeat_update.get("progress", 0.0),
                            current_step=heartbeat_update.get("current_step", "Processing..."),
                            status=heartbeat_update.get("status", "processing"),
                            message=heartbeat_update.get("message", ""),
                            early_preview_available=heartbeat_update.get("early_preview_available", False),
                            early_preview_path=heartbeat_update.get("early_preview_path", ""),
                            current_chunk=heartbeat_update.get("current_chunk", 0),
                            total_chunks=heartbeat_update.get("total_chunks", 0),
                            log_entries=log_entries,
                            # Detailed progress fields from preserved state
                            stage=heartbeat_update.get("stage", ""),
                            stage_number=heartbeat_update.get("stage_number", 0),
                            total_stages=heartbeat_update.get("total_stages", 0),
                            segments_processed=heartbeat_update.get("segments_processed", 0),
                            current_time=heartbeat_update.get("current_time", 0.0),
                            current_time_formatted=heartbeat_update.get("current_time_formatted", ""),
                            total_duration=heartbeat_update.get("total_duration", 0.0),
                            total_duration_formatted=heartbeat_update.get("total_duration_formatted", ""),
                            progress_percent=heartbeat_update.get("progress_percent", 0.0),
                            elapsed_time=heartbeat_update.get("elapsed_time", 0.0),
                        )
                    except Exception as e:
                        logger.error(
                            "Error streaming progress",
                            session_id=session_id,
                            exc_info=True,
                            extra_data={"error": str(e)},
                        )
                        break
            finally:
                # Clean up log handler
                self._remove_log_handler(log_handler)
                # Clear sent log IDs for this session to free memory
                if session_id in _sent_log_ids:
                    del _sent_log_ids[session_id]
                # Clear log buffer after a delay (to allow final logs to be sent)
                # We'll keep it for now in case of reconnection
            
        except Exception as e:
            logger.error(
                "TranslateVideo failed",
                session_id=session_id,
                exc_info=True,
                extra_data={"error": str(e)},
            )
            # Send error response with logs
            log_entries = self._get_log_entries(session_id)
            yield translation_pb2.TranslationProgress(
                session_id=session_id,
                progress=0.0,
                current_step="Error",
                status="failed",
                message=str(e),
                log_entries=log_entries,
            )
    
    async def GetTranslationResult(
        self,
        request: translation_pb2.ResultRequest,
        context: grpc.aio.ServicerContext,
    ) -> translation_pb2.TranslationResult:
        """Get translation result for a session."""
        session_id = request.session_id
        session = self.session_manager.get_session(session_id)
        
        if not session:
            context.set_code(grpc.StatusCode.NOT_FOUND)
            context.set_details(f"Session not found: {session_id}")
            return translation_pb2.TranslationResult()
        
        # Get artifacts
        artifacts = self.path_resolver.get_session_artifacts(session_id)
        
        # Build result
        result = translation_pb2.TranslationResult(
            session_id=session_id,
            output_path=str(artifacts["translated_video"]),
            status=session.get("status", "unknown"),
            original_srt=str(artifacts.get("original_subtitles", "")),
            translated_srt=str(artifacts.get("translated_subtitles", "")),
        )
        
        # Add result data if available
        if "result" in session:
            result_data = session["result"]
            result.processing_time_seconds = result_data.get("processing_time_seconds", 0.0)
            if "artifacts" in result_data:
                artifacts_data = result_data["artifacts"]
                if Path(artifacts_data.get("translated_video", "")).exists():
                    result.output_size = Path(artifacts_data["translated_video"]).stat().st_size
        
        return result
    
    async def CancelTranslation(
        self,
        request: translation_pb2.CancelRequest,
        context: grpc.aio.ServicerContext,
    ) -> translation_pb2.CancelResponse:
        """Cancel a translation session."""
        session_id = request.session_id
        success = self.session_manager.cancel_session(session_id)
        
        return translation_pb2.CancelResponse(
            success=success,
            message="Session cancelled" if success else "Session not found",
        )
    
    async def ControlTranslation(
        self,
        request: translation_pb2.ControlTranslationRequest,
        context: grpc.aio.ServicerContext,
    ) -> translation_pb2.ControlTranslationResponse:
        """Control translation (pause/resume/cancel)."""
        session_id = request.session_id
        action = request.action
        
        success = False
        is_paused = False
        
        if action == "pause":
            success = self.session_manager.pause_session(session_id)
            is_paused = True
        elif action == "resume":
            success = self.session_manager.resume_session(session_id)
        elif action == "cancel":
            success = self.session_manager.cancel_session(session_id)
        
        return translation_pb2.ControlTranslationResponse(
            success=success,
            is_paused=is_paused,
            message=f"Action {action} {'succeeded' if success else 'failed'}",
        )
    
    async def DetectLanguage(
        self,
        request: translation_pb2.LanguageDetectionRequest,
        context: grpc.aio.ServicerContext,
    ) -> translation_pb2.LanguageDetectionResult:
        """Detect language of audio file using Whisper and optionally Polyglot."""
        file_path = Path(request.file_path)
        session_id = getattr(request, 'session_id', 'unknown')
        
        try:
            logger.info(
                f"Detecting language for file: {file_path}",
                session_id=session_id,
            )
            
            # Fast language detection: Extract only first 30 seconds for speed
            # This makes detection much faster (2-5 seconds instead of 30+ seconds)
            temp_audio = None
            try:
                # Create temporary audio file for short sample
                temp_audio = Path(tempfile.gettempdir()) / f"lang_detect_{session_id}.wav"
                
                # Extract first 30 seconds using FFmpeg (fast)
                extract_cmd = [
                    'ffmpeg', '-i', str(file_path),
                    '-t', '30',  # Only first 30 seconds
                    '-ar', '16000',  # 16kHz sample rate (optimal for Whisper)
                    '-ac', '1',  # Mono
                    '-y', str(temp_audio)  # Overwrite if exists
                ]
                
                result = subprocess.run(
                    extract_cmd,
                    capture_output=True,
                    text=True,
                    timeout=10  # 10 second timeout for extraction
                )
                
                if result.returncode != 0:
                    raise RuntimeError(f"FFmpeg extraction failed: {result.stderr}")
                
                if not temp_audio.exists() or temp_audio.stat().st_size == 0:
                    raise RuntimeError("Extracted audio file is empty")
                
                # Use Whisper for fast audio language detection on short sample
                model_manager = get_model_manager()
                whisper_model = await model_manager.get_whisper_model("base")
                
                # Fast language detection on 30-second sample
                segments, info = whisper_model.transcribe(
                    str(temp_audio),
                    language=None,  # Auto-detect
                    word_timestamps=False,
                    vad_filter=True,  # Use VAD to speed up
                    beam_size=1,  # Faster beam size
                    best_of=1,  # Don't try multiple candidates
                    temperature=0.0,  # Deterministic
                )
                
                # Get detected language from Whisper (available in info object)
                detected_lang = info.language if hasattr(info, 'language') else 'en'
                confidence = getattr(info, 'language_probability', 0.9)
                
            finally:
                # Clean up temporary audio file
                if temp_audio and temp_audio.exists():
                    try:
                        temp_audio.unlink()
                    except Exception:
                        pass  # Ignore cleanup errors
            
            return translation_pb2.LanguageDetectionResult(
                detected_language=detected_lang,
                confidence=confidence,
                success=True,
                message=f"Language detected: {detected_lang} (confidence: {confidence:.2f})",
            )
        except Exception as e:
            logger.error(
                f"Language detection failed: {e}",
                session_id=session_id,
                exc_info=True,
            )
        return translation_pb2.LanguageDetectionResult(
            detected_language="en",
                confidence=0.5,
                success=False,
                message=f"Language detection failed: {str(e)}",
        )
    
    async def GetAIInsights(
        self,
        request: translation_pb2.AIInsightsRequest,
        context: grpc.aio.ServicerContext,
    ) -> translation_pb2.AIInsightsResponse:
        """Get AI insights for a session."""
        session_id = request.session_id
        session = self.session_manager.get_session(session_id)
        
        if not session:
            return translation_pb2.AIInsightsResponse(
                session_id=session_id,
                success=False,
                message="Session not found",
            )
        
        # Build insights from session data
        insights = []
        
        if "result" in session:
            result = session["result"]
            if "quality_metrics" in result:
                metrics = result["quality_metrics"]
                insights.append(
                    translation_pb2.AIInsight(
                        id="quality_metrics",
                        type="quality",
                        title="Quality Metrics",
                        description="Translation quality metrics",
                        impact="informational",
                        timestamp=session.get("updated_at", ""),
                        data=str(metrics),
                    )
                )
        
        return translation_pb2.AIInsightsResponse(
            session_id=session_id,
            insights=insights,
            total_insights=len(insights),
            success=True,
            message="Insights retrieved successfully",
        )
    
    async def _get_first_segment(self, segments) -> Optional[str]:
        """
        Get first segment from Whisper segments iterator.
        Helper method for fast language detection.
        
        Args:
            segments: Whisper segments iterator
            
        Returns:
            First segment text or None
        """
        try:
            for segment in segments:
                if segment.text.strip():
                    return segment.text.strip()
        except Exception:
            pass
        return None

