import {
  Controller,
  Get,
  Post,
  Patch,
  Delete,
  Param,
  Body,
  UploadedFile,
  UseInterceptors,
  BadRequestException,
  Sse,
  MessageEvent,
} from '@nestjs/common';
import { FileInterceptor } from '@nestjs/platform-express';
import { diskStorage } from 'multer';
import { extname, resolve } from 'path';
import { exec } from 'child_process';
import { promisify } from 'util';
import {
  Observable,
  interval,
  map,
  takeWhile,
  catchError,
  finalize,
  of,
} from 'rxjs';
import { SessionsService, Session } from './sessions.service';
import { MlClientService } from '../ml-client/ml-client.service';

@Controller()
export class SessionsController {
  // Track sent log IDs per session to avoid duplicates in logs stream
  private sentLogIds: Map<string, Set<string>> = new Map();

  constructor(
    private readonly sessionsService: SessionsService,
    private readonly mlClientService: MlClientService,
  ) {}

  @Get('health')
  health() {
    return {
      status: 'healthy',
      timestamp: new Date().toISOString(),
      activeSessions: this.sessionsService.findAll().length,
    };
  }

  @Get('sessions')
  getAllSessions() {
    return { sessions: this.sessionsService.findAll() };
  }

  @Get('sessions/:id')
  getSession(@Param('id') id: string) {
    return this.sessionsService.findOne(id);
  }

  @Patch('sessions/:id/metadata')
  updateMetadata(@Param('id') id: string, @Body() metadata: any) {
    this.sessionsService.updateProgress(id, { metadata });
    return this.sessionsService.findOne(id);
  }

  @Sse('progress/:id')
  streamProgress(@Param('id') id: string): Observable<MessageEvent> {
    console.log(`Starting SSE stream for session: ${id}`);
    let progressCounter = 0;

    return interval(2000).pipe(
      // Increased interval to 2 seconds
      map(() => {
        const session = this.sessionsService.findOne(id);
        if (!session) {
          return {
            data: JSON.stringify({
              sessionId: id,
              status: 'not_found',
              progress: 0,
              currentStep: 'Session not found',
              message: 'Session not found',
            }),
          };
        }

        // Don't simulate progress - use real progress from gRPC stream
        // The VideoService already handles real progress updates via gRPC
        // This SSE stream should only report the current state, not modify it

        // Use real chunk data from gRPC stream (already updated by VideoService)
        // Prioritize gRPC data over calculated values
        const totalChunks = session.totalChunks || 0; // Use gRPC data, show 0 if not yet available
        const currentChunk = session.currentChunk || 0;

        const progressData = {
          sessionId: id,
          status: session.status,
          progress: session.progress,
          currentStep: session.currentStep,
          message: `Processing ${session.currentStep}...`,
          sourceLang: session.sourceLang,
          targetLang: session.targetLang,
          // Use helper methods for consistent data
          totalChunks,
          currentChunk,
          etaSeconds: this.calculateETA(session),
          processingSpeed: this.calculateProcessingSpeed(session),
          hardwareInfo: this.getHardwareInfo(),
          availableSegments: this.getAvailableSegments(session),
          early_preview_available: session.earlyPreviewAvailable || false,
          early_preview_path: session.earlyPreviewPath || '',
          isPaused: session.isPaused || false,
          logs: session.logs || [], // Include logs in SSE stream
          // Detailed progress fields (from backend)
          stage: session.stage,
          stage_number: session.stage_number,
          total_stages: session.total_stages,
          segments_processed: session.segments_processed,
          current_time: session.current_time,
          current_time_formatted: session.current_time_formatted,
          total_duration: session.total_duration,
          total_duration_formatted: session.total_duration_formatted,
          progress_percent: session.progress_percent,
          elapsed_time: session.elapsed_time,
        };

        // Debug: Log if we're sending logs in progress stream
        if (progressData.logs && progressData.logs.length > 0) {
          console.log(`ðŸ“¡ [PROGRESS STREAM] Sending ${progressData.logs.length} logs for session ${id} in progress update`);
        }

        return {
          data: JSON.stringify(progressData),
        };
      }),
      takeWhile(() => {
        const session = this.sessionsService.findOne(id);
        return session && session.status === 'processing';
      }),
      catchError((error) => {
        console.error(`SSE error for session ${id}:`, error);
        return of({
          data: JSON.stringify({
            sessionId: id,
            status: 'error',
            progress: 0,
            currentStep: 'Error',
            message: error.message,
          }),
        });
      }),
      finalize(() => {
        console.log(`SSE stream ended for session: ${id}`);
      }),
    );
  }

  private getCurrentStep(progress: number): string {
    if (progress <= 10) return 'Initializing';
    if (progress <= 25) return 'Video Analysis';
    if (progress <= 50) return 'Speech-to-Text';
    if (progress <= 75) return 'Translation';
    if (progress <= 90) return 'Text-to-Speech';
    if (progress < 100) return 'Lip-Sync & Finalization';
    return 'Completed';
  }

  @Get('ai-insights/:id')
  async getAIInsights(@Param('id') id: string) {
    try {
      // Try to get real AI insights from Python backend first
      try {
        const aiInsights = await this.mlClientService.getAIInsights(id);

        if (
          aiInsights &&
          aiInsights.insights &&
          aiInsights.insights.length > 0
        ) {
          console.log(
            `Retrieved ${aiInsights.insights.length} real AI insights for session ${id}`,
          );
          return {
            insights: aiInsights.insights,
            sessionId: id,
            source: 'real',
            totalInsights:
              aiInsights.total_insights || aiInsights.insights.length,
          };
        }
      } catch (grpcError) {
        console.warn(
          `Failed to get real AI insights for session ${id}:`,
          grpcError.message,
        );
      }

      // Fallback to mock data if real insights not available
      console.log(
        `Using mock AI insights for session ${id} (real data not available)`,
      );

      const mockInsights = [
        {
          id: '1',
          type: 'decision',
          title: 'Optimal Chunk Size Selected',
          description:
            'AI analyzed video density and selected 30-second chunks for optimal processing speed and quality balance.',
          impact: 'high',
          timestamp: new Date().toISOString(),
          data: {
            chunkSize: 30,
            reasoning: 'Balanced processing speed with quality',
          },
        },
        {
          id: '2',
          type: 'optimization',
          title: 'Voice Model Optimized',
          description:
            'Selected best voice model for target language based on phonetic analysis and naturalness scoring.',
          impact: 'high',
          timestamp: new Date().toISOString(),
          data: { voiceModel: 'en-US-AriaNeural', score: 0.94 },
        },
        {
          id: '3',
          type: 'success',
          title: 'Lip-Sync Accuracy Achieved',
          description:
            'Achieved 98.5% lip-sync accuracy using advanced temporal alignment algorithms.',
          impact: 'high',
          timestamp: new Date().toISOString(),
          data: { accuracy: 0.985, method: 'temporal_alignment' },
        },
      ];

      return {
        insights: mockInsights,
        sessionId: id,
        source: 'mock',
        totalInsights: mockInsights.length,
      };
    } catch (error) {
      console.error('Error getting AI insights:', error);
      return { insights: [] };
    }
  }

  @Get('random-segments/:id')
  getRandomSegments(@Param('id') id: string) {
    try {
      const session = this.sessionsService.findOne(id);

      // Adaptive parameters based on video duration
      const duration = session.metadata?.duration || 300;
      const minGap = duration < 120 ? 5 : 10; // 5s for short, 10s for long
      const minDuration = duration < 120 ? 8 : 10;
      const maxDuration = duration < 120 ? 15 : 25;

      // Generate fewer spots for shorter videos
      let numSegments;
      if (duration < 60) {
        numSegments = 2;
      } else if (duration < 120) {
        numSegments = 3;
      } else {
        numSegments = 5;
      }

      // Early return if video is too short
      if (duration < minDuration * numSegments + minGap * (numSegments - 1)) {
        console.warn(
          `Video too short for ${numSegments} spots. Duration: ${duration}s`,
        );
        return {
          segments: [],
          error: `Video too short for random spots (needs ${minDuration * numSegments + minGap * (numSegments - 1)}s minimum)`,
        };
      }

      const segments = [];
      const timestamps: number[] = [];
      let attempts = 0;
      const maxAttempts = 100; // Increased from 50

      while (timestamps.length < numSegments && attempts < maxAttempts) {
        // Random timestamp between 0 and duration - minDuration
        const timestamp = Math.floor(
          Math.random() * Math.max(1, duration - minDuration),
        );

        // Check if this timestamp doesn't overlap with existing ones
        const hasOverlap = timestamps.some(
          (ts) => Math.abs(ts - timestamp) < minGap,
        );

        if (!hasOverlap) {
          timestamps.push(timestamp);
        }
        attempts++;
      }

      // Sort timestamps to have them in chronological order
      timestamps.sort((a, b) => a - b);

      // Generate segments with adaptive random durations
      for (let i = 0; i < timestamps.length; i++) {
        const timestamp = timestamps[i];
        // Random duration within min-max range, not exceeding video end
        const durationVariation =
          minDuration + Math.random() * (maxDuration - minDuration);
        const actualDuration = Math.min(
          Math.floor(durationVariation),
          duration - timestamp,
        );

        const endTime = timestamp + actualDuration;
        const timeInMinutes = Math.floor(timestamp / 60);
        const timeInSeconds = Math.floor(timestamp % 60);
        const timeStr = `${timeInMinutes}:${timeInSeconds.toString().padStart(2, '0')}`;
        const endMinutes = Math.floor(endTime / 60);
        const endSeconds = Math.floor(endTime % 60);
        const endStr = `${endMinutes}:${endSeconds.toString().padStart(2, '0')}`;

        segments.push({
          id: i + 1,
          timestamp,
          duration: actualDuration,
          title: `Spot ${i + 1} (${timeStr})`,
          originalText: `Random quality check spot from ${timeStr} to ${endStr}. Validating translation accuracy and lip-sync quality at this point in the video.`,
          translatedText: `Ð¡Ð»ÑƒÑ‡Ð°Ð¹Ð½Ð°Ñ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒÐ½Ð°Ñ Ñ‚Ð¾Ñ‡ÐºÐ° Ñ ${timeStr} Ð´Ð¾ ${endStr}. ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð° Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð³ÑƒÐ± Ð² ÑÑ‚Ð¾Ð¼ Ð¼ÐµÑÑ‚Ðµ Ð²Ð¸Ð´ÐµÐ¾.`,
        });
      }

      return { segments };
    } catch (error) {
      console.error('Failed to generate random segments:', error);
      const errorMessage =
        error instanceof Error ? error.message : 'Failed to generate segments';
      return {
        segments: [],
        error: errorMessage,
      };
    }
  }

  @Post('upload')
  @UseInterceptors(
    FileInterceptor('file', {
      storage: diskStorage({
        destination: (req, file, cb) => {
          const path = require('path');
          const fs = require('fs');
          
          // Check if running in Docker (check for /.dockerenv or DOCKER_CONTAINER env var)
          // Don't use NODE_ENV === 'production' as that's also true for standalone AppImage
          const isDocker = fs.existsSync('/.dockerenv') || process.env.DOCKER_CONTAINER === 'true';
          const dockerUploadsPath = '/app/uploads';
          
          // Use environment variable if set (for standalone AppImage)
          if (process.env.UPLOADS_DIR) {
            const uploadsDir = process.env.UPLOADS_DIR;
            if (!fs.existsSync(uploadsDir)) {
              fs.mkdirSync(uploadsDir, { recursive: true });
            }
            cb(null, uploadsDir);
          } else if (isDocker && fs.existsSync('/app/uploads')) {
            // Running in Docker - use /app/uploads
            if (!fs.existsSync(dockerUploadsPath)) {
              fs.mkdirSync(dockerUploadsPath, { recursive: true });
            }
            cb(null, dockerUploadsPath);
          } else {
            // Running locally or standalone - use .data/uploads
            const projectRoot = path.resolve(__dirname, '..', '..', '..');
            const dataDir = path.join(projectRoot, '.data', 'uploads');
            if (!fs.existsSync(path.join(projectRoot, '.data'))) {
              fs.mkdirSync(path.join(projectRoot, '.data'), { recursive: true });
            }
            if (!fs.existsSync(dataDir)) {
              fs.mkdirSync(dataDir, { recursive: true });
            }
            cb(null, dataDir);
          }
        },
        filename: (req, file, cb) => {
          const uniqueSuffix =
            Date.now() + '-' + Math.round(Math.random() * 1e9);
          cb(null, `${uniqueSuffix}${extname(file.originalname)}`);
        },
      }),
      limits: {
        fileSize: 100 * 1024 * 1024 * 1024, // 100GB - supports 15+ hour videos
      },
    }),
  )
  async uploadVideo(
    @UploadedFile() file: Express.Multer.File,
    @Body('sourceLang') sourceLang?: string,
    @Body('targetLang') targetLang?: string,
  ) {
    // FileInterceptor parses FormData, extract language fields using @Body() decorators
    const finalSourceLang = sourceLang || 'en';
    const finalTargetLang = targetLang || 'hy'; // Default to Armenian

    console.log('Upload received - languages from @Body():', {
      sourceLang: finalSourceLang,
      targetLang: finalTargetLang,
    });
    if (!file) {
      throw new BadRequestException('No file uploaded');
    }

    // Convert relative path to absolute path for Python ML service
    // In Docker, file.path is already absolute (/app/uploads/...)
    // In local dev, it's relative (.data/uploads/...), so resolve it
    let absolutePath: string;
    if (file.path.startsWith('/')) {
      // Already absolute (Docker)
      absolutePath = file.path;
    } else {
      // Relative path (local dev) - resolve it
      absolutePath = resolve(file.path);
    }

    const session = this.sessionsService.createSession({
      filePath: absolutePath,
      fileName: file.originalname,
      fileSize: file.size,
      sourceLang: finalSourceLang,
      targetLang: finalTargetLang,
    });

    console.log('Created session with languages:', {
      sourceLang: session.sourceLang,
      targetLang: session.targetLang,
    });

    // Extract actual video duration using FFprobe
    const duration = await this.extractVideoDuration(file.path);
    // Don't calculate totalChunks here - let Python ML service set the correct value via gRPC

    // Update session with calculated fields
    this.sessionsService.updateProgress(session.sessionId, {
      duration,
      metadata: {
        duration,
        resolution: '1920x1080',
        codec: 'H.264',
        size: file.size,
        name: file.originalname,
      },
      // Don't set totalChunks here - let gRPC stream set the correct value
      currentChunk: 0,
      etaSeconds: duration * 2, // 2x realtime processing
      processingSpeed: 2.5,
      hardwareInfo: this.getHardwareInfo(),
      availableSegments: [],
      earlyPreviewAvailable: false,
      earlyPreviewPath: '',
      isPaused: false,
    });

    return {
      ...session,
      // Don't return totalChunks here - let gRPC stream set the correct value
      currentChunk: 0,
      etaSeconds: duration * 2,
      processingSpeed: 2.5,
      hardwareInfo: this.getHardwareInfo(),
      availableSegments: [],
      earlyPreviewAvailable: false,
      earlyPreviewPath: '',
      isPaused: false,
    };
  }

  @Post(':sessionId/detect-language')
  async detectLanguageForSession(@Param('sessionId') sessionId: string) {
    const session = this.sessionsService.findOne(sessionId);

    try {
      // Get the uploaded file path from the session
      const filePath = session.filePath;
      if (!filePath) {
        throw new BadRequestException('No file found in session');
      }

      // Call Python ML gRPC service for language detection
      const result = await this.mlClientService.detectLanguage(filePath);
      return {
        detected_language: result.detected_language || 'en',
        confidence: result.confidence || 0.5,
        success: result.success !== false,
        message: result.message || 'Language detection completed',
      };
    } catch (error) {
      console.error('Language detection error:', error);
      // Return a fallback detection
      return {
        detected_language: 'en',
        confidence: 0.5,
        success: false,
        message: 'Language detection failed, defaulting to English',
      };
    }
  }

  @Post('detect-language')
  @UseInterceptors(
    FileInterceptor('file', {
      storage: diskStorage({
        destination: (req, file, cb) => {
          const path = require('path');
          const fs = require('fs');
          
          // Check if running in Docker (check for /.dockerenv or DOCKER_CONTAINER env var)
          // Don't use NODE_ENV === 'production' as that's also true for standalone AppImage
          const isDocker = fs.existsSync('/.dockerenv') || process.env.DOCKER_CONTAINER === 'true';
          const dockerUploadsPath = '/app/uploads';
          
          // Use environment variable if set (for standalone AppImage)
          if (process.env.UPLOADS_DIR) {
            const uploadsDir = process.env.UPLOADS_DIR;
            if (!fs.existsSync(uploadsDir)) {
              fs.mkdirSync(uploadsDir, { recursive: true });
            }
            cb(null, uploadsDir);
          } else if (isDocker && fs.existsSync('/app/uploads')) {
            // Running in Docker - use /app/uploads
            if (!fs.existsSync(dockerUploadsPath)) {
              fs.mkdirSync(dockerUploadsPath, { recursive: true });
            }
            cb(null, dockerUploadsPath);
          } else {
            // Running locally or standalone - use .data/uploads
            const projectRoot = path.resolve(__dirname, '..', '..', '..');
            const dataDir = path.join(projectRoot, '.data', 'uploads');
            if (!fs.existsSync(path.join(projectRoot, '.data'))) {
              fs.mkdirSync(path.join(projectRoot, '.data'), { recursive: true });
            }
            if (!fs.existsSync(dataDir)) {
              fs.mkdirSync(dataDir, { recursive: true });
            }
            cb(null, dataDir);
          }
        },
        filename: (req, file, cb) => {
          const uniqueSuffix =
            Date.now() + '-' + Math.round(Math.random() * 1e9);
          cb(null, `detect_${uniqueSuffix}${extname(file.originalname)}`);
        },
      }),
      limits: {
        fileSize: 10 * 1024 * 1024 * 1024, // 10GB for detection - supports large videos
      },
    }),
  )
  async detectLanguage(@UploadedFile() file: Express.Multer.File) {
    if (!file) {
      throw new BadRequestException('No file uploaded');
    }

    try {
      // Call Python ML gRPC service for language detection
      // In Docker, file.path is already absolute (/app/uploads/...)
      // In local dev, it's relative (.data/uploads/...), so resolve it
      let filePath: string;
      if (file.path.startsWith('/')) {
        // Already absolute (Docker)
        filePath = file.path;
      } else {
        // Relative path (local dev) - resolve it
        filePath = resolve(file.path);
      }
      const result = await this.mlClientService.detectLanguage(filePath);
      return {
        detected_language: result.detected_language || 'en',
        confidence: result.confidence || 0.5,
        success: result.success !== false,
        message: result.message || 'Language detection completed',
      };
    } catch (error) {
      console.error('Language detection error:', error);
      // Return a fallback detection
      return {
        detected_language: 'en',
        confidence: 0.5,
        message: 'Language detection unavailable, using English as default',
      };
    }
  }

  @Delete('sessions/:id')
  deleteSession(@Param('id') id: string) {
    this.sessionsService.delete(id);
    return { success: true, message: 'Session deleted' };
  }

  @Get('download-info/:id')
  async downloadTranslatedVideo(@Param('id') id: string) {
    try {
      const session = this.sessionsService.findOne(id);
      if (!session) {
        throw new BadRequestException('Session not found');
      }

      // Check if translation is completed
      if (session.status !== 'completed') {
        throw new BadRequestException('Translation not completed');
      }

      // Get the result from the session (check both result and metadata)
      const outputPath =
        session.result?.outputPath || session.metadata?.outputPath;
      if (!outputPath) {
        throw new BadRequestException('No translated video available');
      }

      // Return the file path for the frontend to access
      return {
        success: true,
        outputPath: outputPath,
        fileName: session.fileName || 'translated_video.mp4',
        available: true,
      };
    } catch (error) {
      console.error('Error getting translated video:', error);
      throw new BadRequestException('Translated video not available');
    }
  }

  @Get('stream/:id')
  async streamVideo(@Param('id') id: string) {
    try {
      const session = this.sessionsService.findOne(id);
      if (!session) {
        throw new BadRequestException('Session not found');
      }

      // Check if translation is completed
      if (session.status !== 'completed') {
        throw new BadRequestException('Translation not completed');
      }

      // Get the result from the session
      if (!session.result?.outputPath) {
        throw new BadRequestException('No translated video available');
      }

      // Return the file path for streaming
      return {
        success: true,
        streamPath: session.result.outputPath,
        fileName: session.fileName || 'translated_video.mp4',
        available: true,
      };
    } catch (error) {
      console.error('Error streaming video:', error);
      throw new BadRequestException('Video not available for streaming');
    }
  }

  private calculateETA(session: Session): number {
    // If backend provides ETA, use it (most accurate)
    if (session.etaSeconds && session.etaSeconds > 0) {
      return session.etaSeconds;
    }

    // Use chunk-based calculation for more accuracy with current/totalChunks
    if (
      session.currentChunk !== undefined &&
      session.totalChunks &&
      session.totalChunks > 0
    ) {
      const now = new Date();
      const elapsed = (now.getTime() - session.createdAt.getTime()) / 1000; // seconds
      const chunksProcessed = session.currentChunk;
      const chunksRemaining = session.totalChunks - chunksProcessed;

      if (chunksProcessed > 0 && chunksRemaining > 0) {
        const timePerChunk = elapsed / chunksProcessed;
        const eta = Math.max(0, Math.floor(timePerChunk * chunksRemaining));
        return eta;
      }

      // If we have totalChunks but no chunks processed yet, estimate based on duration
      if (chunksProcessed === 0 && session.totalChunks > 0) {
        return this.estimateETAFromDuration(session);
      }
    }

    // During initialization or early stages, calculate ETA based on video duration
    if (session.progress <= 5 || !session.currentChunk || session.currentChunk === 0) {
      return this.estimateETAFromDuration(session);
    }

    // Fallback to progress-based calculation
    if (session.progress > 0) {
      const now = new Date();
      const elapsed = (now.getTime() - session.createdAt.getTime()) / 1000; // seconds
      const remaining = (100 - session.progress) / session.progress;
      return Math.max(0, Math.floor(elapsed * remaining));
    }

    // Final fallback: estimate from duration
    return this.estimateETAFromDuration(session);
  }

  private estimateETAFromDuration(session: Session): number {
    // Get video duration from session metadata or duration field
    const duration = session.metadata?.duration || session.duration || 0;
    
    if (duration <= 0) {
      // If no duration available, return 0 (frontend will show "Calculating...")
      return 0;
    }

    // Calculate realistic ETA based on processing stages:
    // 1. Initialization: ~10-30 seconds (model loading, setup)
    // 2. STT (Speech-to-Text): ~0.5x realtime (Whisper processing)
    // 3. Translation: ~0.1x realtime (fast model inference)
    // 4. TTS (Text-to-Speech): ~0.3x realtime (voice synthesis)
    // 5. Audio sync: ~0.2x realtime (audio processing)
    // 6. Video encoding: ~0.5x realtime (video rendering with subtitles)
    
    // Total processing factor: ~1.6x realtime (faster than realtime for most stages)
    // But we add overhead for initialization and chunking
    
    const initializationTime = 20; // 20 seconds for model loading
    const processingFactor = 1.8; // 1.8x realtime (conservative estimate)
    const overheadFactor = 1.15; // 15% overhead for chunking, I/O, etc.
    
    const estimatedProcessingTime = duration * processingFactor * overheadFactor;
    const totalETA = Math.ceil(initializationTime + estimatedProcessingTime);
    
    return Math.max(30, totalETA); // Minimum 30 seconds
  }

  private getHardwareInfo() {
    return {
      cpu: 'Intel Core i7-12700K',
      gpu: 'NVIDIA RTX 4080',
      vram_gb: 16,
      ram_gb: 32,
    };
  }

  private getAvailableSegments(session: Session): string[] {
    if (session.status !== 'processing') return [];

    const totalChunks = session.metadata?.duration
      ? Math.ceil(session.metadata.duration / 30)
      : 10;
    const completedChunks = Math.floor((session.progress / 100) * totalChunks);

    return Array.from(
      { length: completedChunks },
      (_, i) => `seg_${String(i + 1).padStart(3, '0')}`,
    );
  }

  private calculateProcessingSpeed(session: Session): number {
    if (session.progress <= 0) return 0;

    const now = new Date();
    const elapsed = (now.getTime() - session.createdAt.getTime()) / 60000; // minutes
    return Math.round(session.progress / elapsed);
  }

  @Sse('logs/stream/:id')
  streamLogs(@Param('id') id: string): Observable<MessageEvent> {
    let logCounter = 0;
    let currentChunk = 0;

    console.log(`ðŸ”µ [LOGS STREAM] Starting logs stream for session ${id}`);
    return interval(2000).pipe(
      // Increased interval to 2 seconds for better readability
      map(() => {
        try {
          const session = this.sessionsService.findOne(id);
          if (!session) {
            console.log(`ðŸ”µ [LOGS STREAM] Session ${id} not found`);
            return {
              data: JSON.stringify({
                timestamp: new Date().toISOString(),
                level: 'error',
                stage: 'system',
                message: 'Session not found',
                sessionId: id,
              }),
            };
          }

          // Check if session is completed or failed - don't stream logs for these
          if (session.status === 'completed' || session.status === 'failed') {
            console.log(`ðŸ”µ [LOGS STREAM] Session ${id} is ${session.status}, no logs to stream`);
            return {
              data: JSON.stringify({
                timestamp: new Date().toISOString(),
                level: 'info',
                stage: 'system',
                message: `Session ${id} is ${session.status}, no logs to stream`,
                sessionId: id,
              }),
            };
          }

          // Calculate total chunks from session metadata or use default
          const totalChunks =
            session.totalChunks ||
            (session.metadata?.duration
              ? Math.ceil(session.metadata.duration / 30)
              : 10);

          // Generate comprehensive process logs based on session progress
          const progress = session.progress || 0;
          const currentStep = session.currentStep || 'Initializing';

          // Use actual current chunk from session, not calculated
          currentChunk =
            session.currentChunk || Math.floor((progress / 100) * totalChunks);

          // Use real logs from session instead of generating synthetic logs
          // This prevents duplicates with the progress stream
          const realLogs = session.logs || [];
          
          // Debug: Always log to see what's happening
          console.log(`ðŸ“¤ [LOGS STREAM] Session ${id}: ${realLogs.length} total logs in session, status: ${session.status}`);
          
          // Only send new logs that haven't been sent yet
          // Track sent log IDs to avoid duplicates
          if (!this.sentLogIds) {
            this.sentLogIds = new Map<string, Set<string>>();
          }
          if (!this.sentLogIds.has(id)) {
            this.sentLogIds.set(id, new Set());
          }
          const sentIds = this.sentLogIds.get(id)!;
          
          // Find new logs (not yet sent)
          const newLogs = realLogs.filter(log => {
            const logId = `${log.timestamp}-${log.message}`;
            if (sentIds.has(logId)) {
              return false; // Already sent
            }
            sentIds.add(logId); // Mark as sent
            return true;
          });
          
          // Debug logging
          console.log(`ðŸ“¤ [LOGS STREAM] Session ${id}: ${realLogs.length} total logs, ${newLogs.length} new logs to send, ${sentIds.size} already sent`);
          
          // If no new logs, don't send anything (avoid duplicate heartbeat logs)
          // The progress stream already sends progress updates, we don't need duplicate heartbeats
          if (newLogs.length === 0) {
            // Return empty data to keep connection alive without creating log entries
            return {
              data: JSON.stringify({
                type: 'heartbeat',
                timestamp: new Date().toISOString(),
              }),
            };
          }
          
          // Send the first new log (one at a time to avoid overwhelming)
          const logToSend = newLogs[0];
          console.log(`ðŸ“¤ [LOGS STREAM] Sending log: ${logToSend.message.substring(0, 50)}...`);
          return {
            data: JSON.stringify({
              id: `${logToSend.timestamp}-${logToSend.message}`,
              timestamp: logToSend.timestamp,
              level: logToSend.level,
              stage: logToSend.stage,
              message: logToSend.message,
              chunkId: logToSend.chunkId,
              sessionId: logToSend.sessionId || id,
              data: logToSend.extraData,
            }),
          };
        } catch (error) {
          return {
            data: JSON.stringify({
              timestamp: new Date().toISOString(),
              level: 'error',
              stage: 'system',
              message: 'Session error',
              details: error instanceof Error ? error.message : 'Unknown error',
              sessionId: id,
            }),
          };
        }
      }),
      takeWhile(() => {
        // Stop streaming when session is completed, failed, or not found
        const session = this.sessionsService.findOne(id);
        const shouldContinue =
          session &&
          session.status !== 'completed' &&
          session.status !== 'failed';

        if (!shouldContinue) {
          console.log(
            `Stopping logs stream for session ${id} - status: ${session?.status || 'not found'}`,
          );
        }

        return shouldContinue;
      }, true), // Include the last event (completion)
      catchError((error) => {
        console.error('SSE Logs stream error:', error);
        return of({
          data: JSON.stringify({
            timestamp: new Date().toISOString(),
            level: 'error',
            stage: 'system',
            message: 'Stream error',
            details: error instanceof Error ? error.message : 'Unknown error',
            sessionId: id,
          }),
        });
      }),
      finalize(() => {
        console.log(`SSE Logs stream closed for session ${id}`);
      }),
    );
  }

  private generateProcessLogs(
    sessionId: string,
    session: Session,
    progress: number,
    currentChunk: number,
    totalChunks: number,
  ) {
    const timestamp = new Date().toISOString();
    const logs = [];

    // Initialization phase (0-10%)
    if (progress <= 10) {
      logs.push(
        {
          timestamp,
          level: 'info',
          stage: 'initialization',
          message: 'Starting video translation process',
          sessionId,
          data: {
            sourceLang: session.sourceLang,
            targetLang: session.targetLang,
            fileName: session.fileName,
            fileSize: session.fileSize,
          },
        },
        {
          timestamp,
          level: 'info',
          stage: 'initialization',
          message: 'Loading AI models and dependencies',
          sessionId,
          data: {
            models: ['Whisper', 'Helsinki-NLP', 'Edge-TTS'],
            status: 'loading',
          },
        },
        {
          timestamp,
          level: 'success',
          stage: 'initialization',
          message: 'Models loaded successfully',
          sessionId,
          data: {
            whisperStatus: 'ready',
            translationModel: 'ready',
            ttsModel: 'ready',
          },
        },
        {
          timestamp,
          level: 'info',
          stage: 'initialization',
          message: 'Analyzing video metadata and structure',
          sessionId,
          data: {
            duration: session.metadata?.duration || 120,
            resolution: session.metadata?.resolution || '1920x1080',
            codec: session.metadata?.codec || 'H.264',
          },
        },
      );
    }

    // Video analysis phase (10-25%)
    if (progress > 10 && progress <= 25) {
      logs.push(
        {
          timestamp,
          level: 'info',
          stage: 'video_analysis',
          message: 'Extracting audio track from video',
          sessionId,
          data: {
            method: 'FFmpeg',
            status: 'in_progress',
            estimatedTime: '30s',
          },
        },
        {
          timestamp,
          level: 'info',
          stage: 'video_analysis',
          message: 'Detecting speech segments and silence',
          sessionId,
          data: {
            algorithm: 'Voice Activity Detection (VAD)',
            chunks: totalChunks,
            status: 'analyzing',
          },
        },
        {
          timestamp,
          level: 'debug',
          stage: 'video_analysis',
          message: 'AI analyzing video content density',
          sessionId,
          data: {
            analysis: 'Dense dialogue detected',
            recommendation: 'Using 30s chunks for optimal sync',
            confidence: 0.94,
          },
        },
      );
    }

    // Speech-to-Text phase (25-50%)
    if (progress > 25 && progress <= 50) {
      const isLowQuality = Math.random() < 0.1; // 10% chance of warning

      logs.push(
        {
          timestamp,
          level: 'info',
          stage: 'speech_to_text',
          message: `Processing chunk ${currentChunk + 1}/${totalChunks} with Whisper`,
          sessionId,
          data: {
            chunkId: `chunk_${String(currentChunk + 1).padStart(3, '0')}`,
            model: 'Whisper Large v3',
            language: session.sourceLang,
            status: 'transcribing',
          },
        },
        {
          timestamp,
          level: isLowQuality ? 'warning' : 'debug',
          stage: 'speech_to_text',
          message: isLowQuality
            ? 'Low audio quality detected, using enhanced processing'
            : 'AI detecting speaker characteristics',
          sessionId,
          data: isLowQuality
            ? {
                warning: 'Low SNR detected',
                action: 'Applying noise reduction',
                impact: 'Minimal',
              }
            : {
                gender: 'female',
                age_range: '25-35',
                accent: 'neutral',
                confidence: 0.87,
              },
        },
        {
          timestamp,
          level: 'success',
          stage: 'speech_to_text',
          message: 'Transcription completed with high accuracy',
          sessionId,
          data: {
            words: 156,
            avgConfidence: 0.92,
            duration: 28.5,
            wer: 0.05,
          },
        },
      );
    }

    // Translation phase (50-75%)
    if (progress > 50 && progress <= 75) {
      const needsCondensation = Math.random() < 0.15; // 15% chance

      logs.push(
        {
          timestamp,
          level: 'info',
          stage: 'translation',
          message: `Translating chunk ${currentChunk + 1}/${totalChunks} text`,
          sessionId,
          data: {
            chunkId: `chunk_${String(currentChunk + 1).padStart(3, '0')}`,
            model: 'Helsinki-NLP opus-mt',
            sourceLang: session.sourceLang,
            targetLang: session.targetLang,
            status: 'translating',
          },
        },
        {
          timestamp,
          level: needsCondensation ? 'warning' : 'debug',
          stage: 'translation',
          message: needsCondensation
            ? 'Translation exceeds slot duration, applying condensation'
            : 'AI optimizing translation for natural speech patterns',
          sessionId,
          data: needsCondensation
            ? {
                technique: 'Content condensation',
                originalLength: '120 words',
                condensedLength: '98 words',
                compressionRatio: 0.82,
              }
            : {
                technique: 'Context-aware translation',
                adjustments: 'Shortened phrases for better timing',
                compressionRatio: 0.85,
              },
        },
        {
          timestamp,
          level: 'success',
          stage: 'translation',
          message: 'Translation quality validated',
          sessionId,
          data: {
            qualityScore: 0.91,
            timingMatch: 0.94,
            naturalness: 0.88,
          },
        },
      );
    }

    // Text-to-Speech phase (75-90%)
    if (progress > 75 && progress <= 90) {
      logs.push(
        {
          timestamp,
          level: 'info',
          stage: 'text_to_speech',
          message: `Generating speech for chunk ${currentChunk + 1}/${totalChunks}`,
          sessionId,
          data: {
            chunkId: `chunk_${String(currentChunk + 1).padStart(3, '0')}`,
            voice: 'en-US-AriaNeural',
            language: session.targetLang,
            status: 'synthesizing',
          },
        },
        {
          timestamp,
          level: 'debug',
          stage: 'text_to_speech',
          message: 'AI matching voice characteristics to original speaker',
          sessionId,
          data: {
            pitch: 'adjusted +12%',
            speed: 'optimized for timing',
            emotion: 'neutral',
            prosody: 'natural',
          },
        },
        {
          timestamp,
          level: 'info',
          stage: 'text_to_speech',
          message: 'Applying audio normalization and enhancement',
          sessionId,
          data: {
            loudness: 'LUFS -23',
            noiseReduction: 'enabled',
            eq: 'optimized for speech',
          },
        },
      );
    }

    // Lip-sync and finalization phase (90-100%)
    if (progress > 90) {
      logs.push(
        {
          timestamp,
          level: 'info',
          stage: 'lip_sync',
          message: `Synchronizing audio with video for chunk ${currentChunk + 1}/${totalChunks}`,
          sessionId,
          data: {
            chunkId: `chunk_${String(currentChunk + 1).padStart(3, '0')}`,
            algorithm: 'Temporal alignment',
            precision: 'Â±100ms',
            status: 'synchronizing',
          },
        },
        {
          timestamp,
          level: 'debug',
          stage: 'lip_sync',
          message: 'AI analyzing lip movements for perfect sync',
          sessionId,
          data: {
            landmarks: 68,
            accuracy: 0.985,
            adjustments: 'minor timing corrections',
          },
        },
        {
          timestamp,
          level: 'info',
          stage: 'finalization',
          message: 'Merging processed chunks into final video',
          sessionId,
          data: {
            chunks: totalChunks,
            method: 'FFmpeg concat',
            status: 'merging',
          },
        },
        {
          timestamp,
          level: 'success',
          stage: 'finalization',
          message: 'Video chunks merged successfully',
          sessionId,
          data: {
            chunksMerged: totalChunks,
            method: 'FFmpeg',
            status: 'completed',
          },
        },
      );
    }

    // Add final success log only when progress is complete
    if (progress >= 100) {
      logs.push({
        timestamp,
        level: 'success',
        stage: 'finalization',
        message: 'ðŸŽ‰ Translation completed successfully!',
        sessionId,
        data: {
          totalDuration: session.metadata?.duration || 120,
          processingTime: '2.5 minutes',
          qualityScore: 0.94,
          status: 'completed',
        },
      });
    }

    // Add some random system logs for realism
    if (Math.random() > 0.7) {
      logs.push({
        timestamp,
        level: 'debug',
        stage: 'system',
        message: 'Monitoring system resources',
        sessionId,
        data: {
          cpu: '45%',
          memory: '2.1GB',
          gpu: '12%',
          temperature: '68Â°C',
        },
      });
    }

    return logs;
  }

  private async extractVideoDuration(filePath: string): Promise<number> {
    try {
      const execAsync = promisify(exec);
      const { stdout } = await execAsync(
        `ffprobe -v quiet -show_entries format=duration -of csv="p=0" "${filePath}"`,
      );
      const duration = parseFloat(stdout.trim());
      return isNaN(duration) ? 120 : Math.round(duration); // Default to 120s if extraction fails
    } catch (error) {
      console.warn(
        `Failed to extract video duration for ${filePath}:`,
        error.message,
      );
      return 120; // Default fallback
    }
  }
}
